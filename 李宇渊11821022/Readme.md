### About me
* **Name**: 李宇渊  
* **Student ID**: 11821022  
* **Topic**: Neural Networks
### Schedule

Task|Due|Done
-|:-:|:-:
1.选择论文|Mar.14|T
2.精读论文|Mar.21|
3.复现论文|Apr.4|
4.完成实验|Apr.11|
5.撰写报告|Apr.18|  
### 选择论文
[FNText: A Fast Neural Model for Efficient Text Classification](Fntext.pdf)  
这篇论文利用神经网络进行文本分类。传统方法大多利用CNN和RNN等结构搭建深度神经网络，这篇论文构建了一个简单高效的三层神经网络。该模型训练时间短，计算资源要求低，准确率也较为出色。  
* **Abstract**
>In recent years,very deep neural models based convolutional neural networks (CNNs) have achieved remarkable results in natural language processing (NLP). However, the computational complexity also largely increasesas the networks go deeper, which causes long training time. To raise the efﬁciency of calculation, this paper focus on shallow neural model and explores a fast neural text classiﬁcation model FNText, which only contains 3 layers,without activation function and stacked time-consuming convolutional layers. Instead of enumerating a bag of bi-grams, we propose a novel method which utilizes average pooling operation along randomly initializing word vectors to obtain bi-gram features. These additional bi-gramfeatures can further improve the performance of FNText. We improve the training speed by ignoring hyperparameters with zero-gradients. Experiments show that FNText can be trained on more than 300 million words in less than 10 minutes using a standard multicore CPU, and achieves competitive results on several large-scale datasets. Sometimes FNText is on par with very deep neural models.
### 精读论文

### 复现论文

### 完成实验

### 撰写报告
